[
    
    {
        "id": "925b2c52-8c21-441f-bb82-9fda9543f81a",
        "type": "article",
        "domaine": "technology",
        "titre": "The modifiers",
        "url": "https://www.theverge.com/creators/638649/small-business-ai-tiktok-gopro-upcycle-refurbish",
        "description": "Technology doesn’t need to die. So much of our consumer habits frame old gadgets, instruments, and concepts as disposable. So The Verge wanted to talk to small businesses, creators, and fans across the world who have a different view. In this week-long series…",
        "source": "The Verge",
        "author": "Verge Staff",
        "date_pub": "2025-04-08T14:44:13Z",
        "contenu": "How creatives and small businesses are building second lives for their favorite tech.\nTechnology doesn’t need to die. So much of our consumer habits frame old gadgets, instruments, and concepts as disposable. So The Verge wanted to talk to small businesses, creators, and fans across the world who have a different view.\nIn this week-long series, we explore unexpected repurposing — and the remix of the old with the new. In New Delhi, technicians are saving e-waste by giving it new life in refurbished laptops. On an island in Bermuda, conservationists are hacking GoPro cameras. On the internet, dedicated forums are keeping a 22-year-old video game alive with new mods, and TikTokers are spicing up literary genres. From Kyrgyzstan to Kansas, these are the modifiers who don’t let go, but look forward.\n\n﻿India’s repair culture gives new life to dead tech.\nThe community continues to evolve the game.\nThese scientists have taken extreme measures to get away from noise.\nThe Bermuda petrel was thought to be extinct for over 300 years. DIY conservation tech is helping to bring it back from the brink.\nArtists are combining traditional methods with AI image generation.\n﻿India’s repair culture gives new life to dead tech.\nThe community continues to evolve the game.\nThese scientists have taken extreme measures to get away from noise.\nThe Bermuda petrel was thought to be extinct for over 300 years. DIY conservation tech is helping to bring it back from the brink.\nArtists are combining traditional methods with AI image generation.\n© 2025 Vox Media, LLC. All Rights Reserved",
        "keywords": [
            "building second lives",
            "DIY conservation tech",
            "small businesses",
            "tech",
            "favorite tech",
            "Bermuda",
            "Bermuda petrel",
            "India ’s repair",
            "DIY conservation",
            "creatives and small"
        ]
    },
    {
        "id": "4c15acb9-25c2-4cab-9f36-2560f0100abc",
        "type": "article",
        "domaine": "technology",
        "titre": "Watch this ultra-detailed animation of the seafloor",
        "url": "https://www.theverge.com/nasa/634086/ocean-floor-map-satellite-nasa-cnes",
        "description": "The world has a more detailed map of the seafloor than ever before thanks to observations taken from space. NASA published a video this week showing a remarkably clear picture of the bottom of the ocean made possible thanks to new satellite technology. The fa…",
        "source": "The Verge",
        "author": "Justine Calma",
        "date_pub": "2025-03-21T20:15:18Z",
        "contenu": "A NASA and CNES satellite lets scientists see the ocean floor in new ways. \nA NASA and CNES satellite lets scientists see the ocean floor in new ways. \nby  Justine Calma\nThe world has a more detailed map of the seafloor than ever before thanks to observations taken from space. NASA published a video this week showing a remarkably clear picture of the bottom of the ocean made possible thanks to new satellite technology.\nThe face of the moon has been more thoroughly mapped than the depths of Earth’s oceans. But after NASA and French space agency CNES launched the Surface Water and Ocean Topography (SWOT) satellite in 2022, we’re starting to get a clearer look at what lies in the deep. In December, researchers published a revelatory seafloor map in the journal Science using one year of SWOT data. \nThe face of the moon has been more thoroughly mapped than the depths of Earth’s oceans\nThe satellite helps fill in vast gaps in data collected by ship, and offers a higher resolution picture than previous satellites have been able to provide. On a practical level, the maps can help submarines navigate more safely through previously mysterious ocean terrains. They can also inform the precarious work of laying down and repairing underwater telecommunication cables that keep people connected across our planet. \nNASA’s new video is also just fun to watch. The animation shows what the seafloor looks like off the coasts of Mexico, South America, and the Antarctic Peninsula. Research using data from SWOT is ongoing, so we can expect more insights into the ocean’s abyss in the future.\nThe maps use gravity-based data to reveal features researchers had never seen before. Because of their larger mass, rolling abyssal hills and undersea volcanoes called seamounts exert stronger gravitational pull than their surroundings. SWOT can note those subtle differences by observing bumps above along the surface of the sea. In the video, regions colored green are higher relative to purple-colored regions.\nThis method allows SWOT to detect abyssal hills and other features that were too small for older satellites to find using radar pulses. “We were surprised that SWOT could see them so well,” Yao Yu, an oceanographer at Scripps Institution of Oceanography and lead author of the Science paper, said in a NASA blog published this week. Researchers now know that those hills, formed where tectonic plates pull apart from each other, blanket about 70 percent of the ocean floor. That makes them the most common landform on Earth, according to Yu.\nShips using sonar can also map abyssal hills, but it’s a slow and difficult task. To date, ships have only mapped about a quarter of the planet’s ocean floor. \nSWOT was designed to do much more than map the seafloor. Its main purpose is to measure the height of water over oceans, rivers, lakes, and other bodies of water, a mission to take the first complete survey of Earth’s surface water. \nA weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.\n© 2025 Vox Media, LLC. All Rights Reserved",
        "keywords": [
            "CNES satellite",
            "ocean floor",
            "NASA",
            "ocean",
            "NASA and CNES",
            "SWOT",
            "Justine Calma",
            "CNES",
            "satellite",
            "Earth"
        ]
    },
    {
        "id": "5bdc6c69-46e5-4adb-a5fe-720cbdf4fbd6",
        "type": "article",
        "domaine": "technology",
        "titre": "7 cool indie games from GDC 2025",
        "url": "https://www.theverge.com/games/633885/gdc-2025-best-indie-games",
        "description": "The Game Developers Conference, an annual gathering that brings thousands of game developers to San Francisco, has just wrapped up. While most of the event is about networking and interesting talks about game design and technology, there were also a bunch of …",
        "source": "The Verge",
        "author": "Jay Peters, Andrew Webster",
        "date_pub": "2025-03-21T17:12:34Z",
        "contenu": "Some strange and fun things to look forward to.\nby  Jay Peters and  Andrew Webster\nIf you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.\nSome strange and fun things to look forward to.\nby  Jay Peters and  Andrew Webster\nIf you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.\nThe Game Developers Conference, an annual gathering that brings thousands of game developers to San Francisco, has just wrapped up. While most of the event is about networking and interesting talks about game design and technology, there were also a bunch of unreleased indie games that we got to check out. Yes, much of video games in 2025 will probably be defined by the Nintendo Switch 2 and Grand Theft Auto VI. But based on the games we played, there might be some really fun smaller titles that could be huge hits this year, too.\nHere are a few of our favorites that we’re looking forward to.\nDreams of Another is a shooter, but you use your gun to create the world around you instead of destroying it. The game’s graphics have a dreamy, deconstructed quality to them, and when you shoot things, the colorful, unfocused scenes you see will solidify into things like buildings and people. Unexpectedly, my brief demo ended with a more traditional shootout against a sentient manhole cover. (I even lobbed a couple grenades to try and destroy it.) But I’m intrigued by the overall concept, and I look forward to seeing what developer Q-Games does with it in the final version of the game.—JP\nIn Faraway, you play as a shooting star and try to make constellations. The game requires just pressing and holding one button to spin around other stars, create a trail, and essentially play a celestial game of connect the dots. It’s a simple concept, but I was impressed by it immediately. In just a 10-minute demo, I was really starting to get the hang of using my shooting star’s momentum to perfectly swing around a star to make elaborate and high-scoring constellations. That’s all the time it took to make Faraway far and away my favorite game of the show.—JP\nHaste: Broken Worlds is kind of like a 3D version of the classic mobile game Tiny Wings. You play as Zoe, a girl who runs forward at blinding speed through procedurally generated levels. Like with Tiny Wings, there are rolling hills you can launch from for added height, and if you time your fall right into the curve of another hill, you’ll get a satisfying “perfect” message and a boost to a meter that lets you pull out a hoverboard. The game has a roguelike structure, so you’ll get to pick and choose different types of levels to tackle as you make your way through a run. On the show floor, I was awful, dying within the first few levels before I had to hand the controller to the next person in line. But as soon as I got back to my hotel room, I downloaded the game’s free demo on Steam.—JP\nHerdling tasks you with herding mysterious, fluffy beasts. In my demo, the character starts in a dark city before quickly coming upon three adorable beasts that reminded me of a big goat mixed with Sesame Street’s Mr. Snuffleupagus. Your goal is to get them out of the city and out to wilder pastures. You coax the animals forward from behind with a magic stick, and while they generally follow your directions, they are sometimes difficult to direct. More than once, my herd of three walked right into a wall. By the end of the demo, however, I really started to care for my fluffy friends, especially since you can name them — though I can only imagine the cruel ways the developers will pull on my heartstrings over the course of the full adventure.—JP\nThis is sort of the video game equivalent of a supergroup. Goichi “Suda51” Suda (best known for off-kilter games like Killer 7 and No More Heroes) has teamed up with Hidetaka “Swery65” Suehiro (the writer and director behind Deadly Premonition and, more recently, The Good Life) on an extremely challenging and roguelike game set in a world inspired by slasher movies. In the short time I had with the game, I died multiple times, but the twitchy action felt very satisfying, especially once I got the hang of the controls. There’s a neat feature where you can fight alongside a ghost image of your previous run, which seems like it will come in handy. What I didn’t get to experience was the inevitable surreal twist the two creators have put on the horror setting, but the core is solid.—AW\nSlapstick comedy games are really coming into their own of late. And if you loved Untitled Goose Game or Thank Goodness You’re Here, this is definitely one to keep an eye on. The name really says it all: you control an unseemly long and stretchy arm and attempt to... do things, most of which are illegal. The level I played took place on a moving train, and you were tasked with stealing items from passengers while remaining undetected (probably because of how terrifying the arm is). There’s a puzzle element to it; at one point, I needed to take a briefcase from a sleeping businessman, only to find it was chained to his wrist. It’s absurd, of course, but that’s exactly the appeal.—AW\nTo a T is a delightful game about a teenager permanently stuck in a T pose. The game is from Katamari series creator Keita Takahashi, so, of course, it’s wacky and silly. My demo featured an elaborate tooth-brushing sequence, musical numbers, and a talking giraffe. But the demo also hinted at the character’s fear of school and bullying over their T-pose condition. I was surprised how much that made me care for the character, and if the story sticks the landing, it could make a game that might have been pure silliness something more thoughtful. It’s out on May 28th.—JP\nA weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.\n© 2025 Vox Media, LLC. All Rights Reserved",
        "keywords": [
            "Game",
            "Vox Media",
            "Andrew Webster",
            "Jay Peters",
            "Peters and Andrew",
            "Game Developers Conference",
            "game Tiny Wings",
            "games",
            "Verge link",
            "Game Developers"
        ]
    },
    {
        "id": "a4cdee68-48fc-40b7-a3bb-766362061922",
        "type": "article",
        "domaine": "technology",
        "titre": "The Nissan Leaf lives on as a compact SUV with a Tesla charge port",
        "url": "https://www.theverge.com/news/636386/nissan-leaf-third-generation-ev-suv-nacs",
        "description": "The Nissan Leaf is back, and it’s not a frumpy looking hatchback anymore.  The Japanese automaker is dusting off its pioneering EV and giving it new technology and a new form factor. The Leaf will return as a crossover SUV with a Tesla plug (!!), casting off …",
        "source": "The Verge",
        "author": "Andrew J. Hawkins",
        "date_pub": "2025-03-26T14:58:17Z",
        "contenu": "The rumors of my demise were greatly exaggerated.\nThe rumors of my demise were greatly exaggerated.\nby  Andrew J. Hawkins\nThe Nissan Leaf is back, and it’s not a frumpy looking hatchback anymore. \nThe Japanese automaker is dusting off its pioneering EV and giving it new technology and a new form factor. The Leaf will return as a crossover SUV with a Tesla plug (!!), casting off its outdated appearance that previously led to rumors of its inevitable demise. \nNissan is also rebooting some other familiar nameplates, including the Sentra and Rogue. But the newly refreshed, third-generation Leaf is coming first, arriving in North America in 2026.\nIt makes sense that Nissan would throw out its old looks for something new and daring. The automaker is going through a particularly rough patch at the moment, with declining sales and the collapse of a merger deal with Honda. It needs some fresh products to help boost its overall image.\nIt’s hard to tell from the photos, but InsideEVs (which attended the reveal event in Japan) describes the new Leaf as egg-shaped (which is typical for most modern EVs) and about 3/4ths the size of a Tesla Model Y. \nNissan didn’t reveal much about the new Leaf, aside from confirming that it will be built on the modular CMF-EV platform that also undergirds the Ariya EV. The new Leaf will also have “significant range improvements over the previous generation.” The current Leaf sits in the low category for range, with the SV Plus trim getting a maximum 212 miles on single charge. \nPerched on 19-inch alloy wheels, the new Leaf features a panoramic moonroof and — perhaps most shockingly — a native North American Charging Standard (NACS) charging port for Tesla Supercharger access. Nissan says it will be the first EV in its vehicle lineup to roll out of the factory with an integrated NACS port. As one of the first commercially available electric vehicles, the Leaf has always been saddled with outdated charging tech in the form of the CHAdeMO port. But now Nissan is rectifying that issue for this new generation. \nThe Leaf appeared to be headed for the compost heap up until recently, with Automotive News reporting in 2022 that Nissan would be winding down production on the current generation by mid-decade, with no immediate plans for a refresh.\nNissan is promising more details on the new Leaf by mid-year. The company also announced that new ICE and plug-in hybrid versions of its Rogue SUV, and refreshed models of its mid-sized Pathfinder SUV and Sentra sedan, would be coming soon. For Infiniti, a refreshed three-row luxury QX60 SUV is on its way, alongside a new Sport package for the full-size QX80 SUV. \nAnd then in late fiscal year 2027, Nissan will introduce a new, unnamed EV slated for production as its Canton, Mississippi plant. The automaker says it will be an “adventure themed” SUV, so expect something of the Rivian/Scout/Jeep variety. In fiscal year 2028, Infiniti will release a luxury SUV inspired by the Vision QXe concept. \nThe automaker had previously discussed its plans to electrify half its vehicle lineup, including seven EVs for the US and Canada. Today, Nissan clarified that these future models will feature batteries manufactured in the US in partnership with SK one.\nIn Europe, Nissan said its reviving the Micra — a small car the company stopped making in the UK in 2010 — later this year, reintroducing the model as a compact EV. An electric variant of the Juke compact SUV will be launched in fiscal 2026.\nA weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.\n© 2025 Vox Media, LLC. All Rights Reserved",
        "keywords": [
            "Leaf",
            "Nissan",
            "SUV",
            "Nissan Leaf",
            "Andrew J. Hawkins",
            "greatly exaggerated",
            "demise were greatly",
            "rumors",
            "demise",
            "Rogue SUV"
        ]
    },
    {
        "id": "4b1950fc-454a-4feb-b3cc-fee504f8c499",
        "type": "article",
        "domaine": "technology",
        "titre": "All of the updates about OpenAI",
        "url": "https://www.theverge.com/news/640086/openai-chat-gpt-news-updates",
        "description": "What was once a humble research lab has transformed into one of the biggest consumer technology companies of all time. OpenAI, founded in 2015 to develop artificial general intelligence (AGI) — AI systems with human-level intelligence — has transformed dramat…",
        "source": "The Verge",
        "author": "Kylie Robison",
        "date_pub": "2025-04-01T19:49:20Z",
        "contenu": "by  Kylie Robison\nWhat was once a humble research lab has transformed into one of the biggest consumer technology companies of all time.\nOpenAI, founded in 2015 to develop artificial general intelligence (AGI) — AI systems with human-level intelligence — has transformed dramatically since launching ChatGPT, which was once considered to be “the fastest-growing consumer application in history.” Most cofounders have left either to create a competitor or work for one. The company has secured billions in funding and partnerships with Apple and Microsoft, even announcing a $500 billion datacenter project called Stargate. Meanwhile, it faces copyright lawsuits from authors and news organizations, legal action from cofounder Elon Musk over the company’s alleged departure from its original mission, and criticism for burning through cash despite projected billions in revenue. After Altman’s brief ouster, OpenAI is now expected to restructure from a nonprofit-led organization to a full for-profit company to stabilize operations and reassure investors.\nAs San Francisco’s hottest AI company continues to barrel towards ever growing valuations, its claims become more nebulous. Altman expects we may see “the first AI agents ‘join the workforce’ and materially change the output of companies” in 2025, and says his team is “now confident we know how to build AGI as we have traditionally understood it.” A decade since it was founded, OpenAI has become synonymous with the future of AI, with the tech industry and beyond closely monitoring its next moves.\nAll of the news and updates about OpenAI continue below.\nKylie Robison\nBob McGrew first joined OpenAI in 2017, shortly after it had been founded, eventually rising through the ranks to become the company’s chief research officer. Last November, he suddenly departed the startup along with its CTO, Mira Murati. Now, it looks like McGrew has joined Murati at her new AI competitor, Thinking Machine Labs.\n[businessinsider.com]\nKylie Robison\nTwo leading AI labs, OpenAI and Anthropic, just announced major initiatives in higher education. It’s the constant one-upping we’ve all become familiar with: this week, Anthropic dropped their announcement at 8 AM Wednesday, while OpenAI followed with nearly identical news at 8 AM Thursday.\nFor Anthropic, this week’s announcement was its first major academic push. It launched Claude for Education, a university-focused version of its chatbot. The company also announced partnerships with Northeastern University, London School of Economics (LSE), and Champlain College, along with with Internet2, which builds university tech infrastructure, and Instructure (maker of Canvas) to increase “equitable access to tools that support universities as they integrate AI.”\nKylie Robison\nOpenAI has raised $40 billion in a new investment round led by SoftBank, vaulting the company to a $300 billion valuation. It’s the largest funding round for a private tech company in history, according to CNBC.\nOpenAI is set to receive $10 billion up front (SoftBank will invest $7.5 billion along with $2.5 billion “from an investor syndicate,“ according to Bloomberg). The remaining $30 billion is slated to arrive by year’s end, CNBC reported — but only if it officially converts into a for-profit company by then. If not, it reportedly stands to lose a quarter of the deal.\nAdi Robertson\nWhen I saw my colleague Kylie Robison’s story about OpenAI’s new image generator on Tuesday, I thought this week might be fun. Generative AI images raise all kinds of ethical issues, but I find them wildly entertaining, and I spent large chunks of that day watching other Verge staff test ChatGPT in ways that covered the entire spectrum, from cute to cursed.\nBut on Thursday afternoon, the White House decided to spoil it. Its X account posted a photograph of a crying detainee that it bragged was an arrested fentanyl trafficker and undocumented immigrant. Then it added an almost certainly AI-generated cartoon of an officer handcuffing the sobbing woman — not attributed to any particular tool, but in the unmistakable style of ChatGPT’s super-popular Studio Ghibli imitations, which have flooded the internet over the past week.\nChris Welch\nThe fervor around ChatGPT’s more accessible (and more advanced) image generation capabilities has forced OpenAI to “temporarily” put a rate limit on image generation requests, according to CEO Sam Altman. “It’s super fun seeing people love images in ChatGPT, but our GPUs are melting,” he posted on X today. Altman didn’t specify what the rate limit is, but said the safeguard “hopefully” won’t need to be in place for very long as OpenAI tries to increase its efficiency in handling the avalanche of requests.\nThe demand crunch already caused the artificial intelligence company to push back availability of the built-in image generator for users on ChatGPT’s free tier. But apparently that measure alone wasn’t enough to ease the stress on OpenAI’s infrastructure. (Altman said free users will “soon” be able to generate up to three images per day.)\nKylie Robison\nAI-generated images have made significant progress since the days of abstract renderings and glitchy amalgamations. OpenAI’s newly released “Images for ChatGPT” has an uncanny ability to nail depth, shadows, and even text. It’s unleashed a frenzy of people recreating a familiar style: Hayao Miyazaki’s work at Studio Ghibli. The art style was already ubiquitous across the internet, thanks to its comforting, soft aesthetic (just look at Lofi girl) — and now, it’s a fully automated formula.\nThe trend kicked off pretty wholesomely. Couples transformed portraits, pet owners generated cartoonish cats, and many people are busily Ghibli-fying their families (I’ve stuck to selfies, not wanting to share with OpenAI my siblings’ likenesses). It’s an AI-generated version of the human-drawn art commissions people offer on Etsy — you and your loved ones, in the style of your favorite anime.\nEmma Roth\nOpenAI is pushing back the rollout of ChatGPT’s built-in image generator for free users. In a post on Wednesday, CEO Sam Altman admitted that the image-generation tool is more popular than he expected, adding that “rollout to our free tier is unfortunately going to be delayed for awhile.”\nOpenAI only just added image generation capabilities to ChatGPT on Tuesday, allowing users to create images directly within the app using the company’s reasoning model, GPT-4o. Since its launch, users have flooded social media feeds with photos transformed into images generated in the style of Studio Ghibli, a trend that even Altman has gotten in on.\nKylie Robison\nOpenAI is integrating new image generation capabilities directly into ChatGPT starting today — this feature is dubbed “Images in ChatGPT.” Users can now use GPT-4o to generate images within ChatGPT itself.\nThis initial release focuses solely on image creation and will be available across ChatGPT Plus, Pro, Team, and Free subscription tiers. The free tier’s usage limit is the same as DALL-E, spokesperson Taya Christianson told The Verge, but added that they “didn’t have a specific number to share” and ”these may change over time based on demand.“ Per the ChatGPT FAQ, free users were previously able to generate “three images per day with DALL·E 3.” As for the fate of DALL-E, Christianson said “fans” will “still have access via a custom GPT.”\nKylie Robison\nIn a significant executive shuffle announced Monday, OpenAI is expanding COO Brad Lightcap’s responsibilities while CEO Sam Altman shifts his attention more toward the company’s technical direction. The news was first reported by Bloomberg.\nLightcap will now “oversee day-to-day operations,” international expansion, and manage key partnerships with tech giants like Microsoft and Apple, according to Bloomberg. OpenAI has also promoted Mark Chen to chief research officer (he was recently SVP of research) and Julia Villagra to chief people officer (she was formerly VP of people).\nDominic Preston\nA privacy complaint has been filed against OpenAI by a Norwegian man who claims that ChatGPT described him as a convicted murderer who killed two of his own children and attempted to kill a third.\nArve Hjalmar Holmen says that he wanted to find out what ChatGPT would say about him, but was presented with the false claim that he had been convicted for both murder and attempted murder, and was serving 21 years in a Norwegian prison. Alarmingly, the ChatGPT output mixes fictitious details with facts, including his hometown and the number and gender of his children.\nTina Nguyen\nWhen AI giant OpenAI submitted its “freedom-focused” policy proposal to the White House’s AI Action Plan last Thursday, it gave the Trump administration an industry wishlist: use trade laws to export American AI dominance against the looming threat of China, loosen copyright restrictions for training data (also to fight China), invest untold billions in AI infrastructure (again: China), and stop states from smothering it with hundreds of new laws.\nBut specifically, one law: SB 1047, California’s sweeping, controversial, and for now, defeated AI safety bill.\nElizabeth Lopatto\nChatbots can’t think, and increasingly I am wondering whether their makers are capable of thought as well.\nIn mid-February OpenAI released a document called a model spec laying out how ChatGPT is supposed to “think,” particularly about ethics. (It is an update of a much shorter version published last year.) A couple of weeks later, people discovered xAI’s Grok suggesting its owner Elon Musk and titular President Donald Trump deserved the death penalty. xAI’s head of engineering had to step in and fix it, substituting a response that it’s “not allowed to make that choice.” It was unusual, in that someone working on AI made the right call for a change. I doubt it has set precedent.\nTom Warren and Emma Roth\nOpenAI is launching GPT-4.5 today, its newest and largest AI language model. GPT-4.5 will be available as a research preview for ChatGPT Pro users to start. OpenAI is calling the release its “most knowledgeable model yet,” but initially warned that GPT-4.5 is not a frontier model and might not perform as well as o1 or o3-mini.\nGPT-4.5 will have better writing capabilities, improved world knowledge, and what OpenAI calls a “refined personality over previous models.” OpenAI says interacting with GPT 4.5 will feel more “natural,” adding that the model is better at recognizing patterns and drawing connections, making it ideal for writing, programming, and “solving practical problems.”\nDavid Pierce\nEvery day for the last few weeks, I’ve received a notification on my phone at 7:30 in the morning. The notification comes from ChatGPT, and it always contains the same thing: instructions for a 20-minute full-body workout and a 10-minute meditation. The instructions are simple, and I’ve actually come to appreciate the daily prodding. I do wish it would stop recommending the exact same thing every damn day, though. The mountain climbers and positive intentions are getting a little old.\nOpenAI has added a number of new features to ChatGPT in the last few weeks, a couple of which attempt to turn the chatbot into a straightforward productivity app. There’s Tasks, which all paid users can access and allows you to set reminders and make to-do lists in ChatGPT; and there’s Operator, a so-called “agentic” model for Pro subscribers that attempts to actually accomplish tasks on your behalf. As an incorrigible tester of to-do list apps, I decided to throw my life into ChatGPT and see if it could help me get more done.\nKylie Robison\nAfter her sudden departure from OpenAI last fall, ex-CTO Mira Murati vanished from public view to start something new. Now, she is ready to share some details about what she’s working on.\nHer new AI startup is called Thinking Machines Lab, and while the specifics of what it plans to release are still under wraps, the company says its goal is “to make AI systems more widely understood, customizable and generally capable.” The startup also promises at least some level of public transparency by pledging to regularly publish technical research and code.\nKylie Robison\nOpenAI is releasing a significantly expanded version of its Model Spec, a document that defines how its AI models should behave — and is making it free for anyone to use or modify.\nThe new 63-page specification, up from around 10 pages in its previous version, lays out guidelines for how AI models should handle everything from controversial topics to user customization. It emphasizes three main principles: customizability; transparency; and what OpenAI calls “intellectual freedom” — the ability for users to explore and debate ideas without arbitrary restrictions. The launch of the updated Model Spec comes just as CEO Sam Altman posted that the startup’s next big model, GPT-4.5 (codenamed Orion), will be released soon.\nJay Peters\nOpenAI CEO Sam Altman detailed plans for the company’s GPT-4.5 and GPT-5 AI models in a roadmap published on X on Wednesday.\nIn the post, Altman also acknowledged that OpenAI’s product lineup has gotten complicated and says that the company wants to do “a much better job” simplifying its offerings. “We hate the model picker as much as you do and want to return to magic unified intelligence,” Altman says.\nDavid Pierce\nOver the last few weeks, OpenAI has done the previously unthinkable: it has consistently shipped interesting new user-facing products. First there was Tasks, a way to engage ChatGPT in helping you get things done. Then there was Operator, a way for the chatbot to actually do things for you. And finally there was “deep research,” an extremely imperfect but still very interesting tool for generating deep dives.\nSubscribe: Spotify | Apple Podcasts | Overcast | Pocket Casts | More\nKylie Robison\nOpenAI just made its Super Bowl debut with a 60-second spot that positions AI alongside humanity’s greatest innovations.\nThe commercial traces humanity’s technological evolution through a distinctive pointillism-inspired animation style, transforming abstract dots into iconic images of progress – from early tools like fire and the wheel to modern breakthroughs like DNA sequencing and space exploration. It culminates with modern AI applications, showing ChatGPT handling everyday tasks like drafting business plans and language tutoring. The ad cost roughly $14 million for the first-half placement.\nAdi Robertson\nIn the vast number of fields where generative AI has been tested, law is perhaps its most glaring point of failure. Tools like OpenAI’s ChatGPT have gotten lawyers sanctioned and experts publicly embarrassed, producing briefs based on made-up cases and nonexistent research citations. So when my colleague Kylie Robison got access to ChatGPT’s new “deep research” feature, my task was clear: make this purportedly superpowerful tool write about a law humans constantly get wrong.\nCompile a list of federal court and Supreme Court rulings from the last five years related to Section 230 of the Communications Decency Act, I asked Kylie to tell it. Summarize any significant developments in how judges have interpreted the law.\nEmma Roth\nChatGPT no longer requires you to log in to use the AI chatbot’s search engine, OpenAI announced on Wednesday. With the feature, ChatGPT will surface responses based on information from the web while presenting a list of sources it used to inform its answer.\nOpenAI first launched its search engine to paid ChatGPT subscribers last October and later rolled it out to everyone in December. But now that you no longer need an account to use it, ChatGPT search will compete directly with search engines like Google and Bing.\nEmma Roth\nOpenAI just gave itself a full rebrand, complete with a new typeface, logo, and color palette, as explained to Wallpaper in an interview about the process behind the changes. You’ll have to look closely to spot the difference between the redrawn logo and its old one, but a side-by-side comparison shows the updated “blossom” with a slightly larger space in the center and cleaner lines.\nThough the original logo was designed by OpenAI co-founders Greg Brockman and Ilya Sutskever, an in-house design team led by Veit Moeller and Shannon Jager took the reins this time around, intending to create a “more organic and more human” identity, Wallpaper reports.\nJay Peters\nSoftBank isn’t just part of the group investing $500 billion into Project Stargate to build American AI infrastructure capacity for OpenAI. It’s also making a Japanese joint venture with OpenAI, will spend $3 billion deploying OpenAI tech across SoftBank companies, and claims it will revolutionize business with AI agents through a new AI system called “Cristal intelligence.”\nSoftBank CEO Masayoshi Son, who also invested billions in WeWork, claimed at an announcement event in Tokyo that artificial general intelligence, or AGI will come “much earlier” than his previous two- to three-year prediction, The Wall Street Journal reports. That could be helped, of course, by changes in the definition of AGI, as explained recently by his new partner, Sam Altman.\nRichard Lawler\nOpenAI has revealed another new agentic feature for ChatGPT called deep research, which it says can operate autonomously to “plan and execute a multi-step trajectory to find the data it needs, backtracking and reacting to real-time information where necessary.”\nInstead of simply generating text, it shows a summary of its process in a sidebar, with citations and a summary showing the process used for reference.\nTom Warren\nOpenAI CEO Sam Altman teased exactly two weeks ago that o3-mini would ship in “a couple of weeks,” and it’s arriving on time today. OpenAI is launching its latest o3-mini reasoning model inside ChatGPT and its API services and making a version with rate limits available to free users of ChatGPT for the first time.\nOriginally announced as part of OpenAI’s 12 days of “ship-mas” in December, o3-mini is designed to match o1’s performance in math, coding, and science, while responding faster than the existing reasoning model. OpenAI says o3-mini should respond 24 percent faster than o1-mini and provide more accurate answers in the process. Much like o1-mini, this latest model will show how it worked out an answer, rather than just providing a response.\nA weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.\n© 2025 Vox Media, LLC. All Rights Reserved",
        "keywords": [
            "CEO Sam Altman",
            "Kylie Robison",
            "OpenAI",
            "ChatGPT",
            "Sam Altman",
            "OpenAI CEO Sam",
            "CEO Sam",
            "Altman",
            "Robison",
            "company"
        ]
    },
    {
        "id": "09de0544-23f5-400c-bc8e-ba1eef27e732",
        "type": "article",
        "domaine": "technology",
        "titre": "Android 16 is getting a major hearing accessibility feature",
        "url": "https://www.theverge.com/tech/628533/android-16-auracast-hearing-aid-support",
        "description": "Android phones will soon support Auracast with Bluetooth LE hearing aids, letting people tune in to audio broadcasts in places where it’s otherwise hard to hear. Auracast is a Bluetooth Audio LE feature, and it allows one broadcaster to connect to a virtually…",
        "source": "The Verge",
        "author": "Allison Johnson",
        "date_pub": "2025-03-13T16:00:15Z",
        "contenu": "Auracast will give people who use hearing aids a little extra clarity.\nAuracast will give people who use hearing aids a little extra clarity.\nby  Allison Johnson\nAndroid phones will soon support Auracast with Bluetooth LE hearing aids, letting people tune in to audio broadcasts in places where it’s otherwise hard to hear. Auracast is a Bluetooth Audio LE feature, and it allows one broadcaster to connect to a virtually unlimited number of Bluetooth LE devices. Among other things, the technology can help people who use hearing aids connect directly to audio streams like the public announcement system at an airport, or to an audio feed at a concert venue. At launch, Samsung Galaxy phones running One UI 7 and Google Pixel 9 phones with the Android 16 beta will support it.\nAuracast has been around since the Bluetooth LE spec was completed in 2022. Samsung’s recent Galaxy phones already support sharing audio to other devices via Auracast, and it looks as though Google will add similar audio sharing capabilities in Android 16. Hearing aid support adds another layer of functionality, and on Pixel 9 phones connecting to a broadcast will be as simple as scanning a QR code. Otherwise, you can access a public broadcast through your settings menu in basically the same way you connect to a Wi-Fi network.\nGoogle is also announcing today that Android 16 has reached platform stability, moving it one step closer to a full launch. It first debuted in developer preview in November of last year, aligning with Google’s plan to shift its Android release schedule forward this year to Q2. With I/O just around the corner, there’s a lot brewing in Mountain View.\nA weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.\n© 2025 Vox Media, LLC. All Rights Reserved",
        "keywords": [
            "hearing aids",
            "give people",
            "hearing aids connect",
            "Allison Johnson",
            "Hearing aid support",
            "Auracast",
            "hearing",
            "Bluetooth",
            "extra clarity",
            "audio"
        ]
    },
    {
        "id": "8f410d4d-84a1-4ef0-8387-ef42e9a681c2",
        "type": "article",
        "domaine": "technology",
        "titre": "Hear what Horizon Zero Dawn actor Ashly Burch thinks about AI taking her job",
        "url": "https://www.theverge.com/news/630176/ashly-burch-sony-ai-horizon-aloy-tech-demo-sag-aftra-strike",
        "description": "Ashly Burch, the award-winning voice and performance actor behind Horizon Zero Dawn’s Aloy — one of the most prominent characters on PlayStation today — has some news and some very strong thoughts about the leaked Sony experiment that saw her character voiced…",
        "source": "The Verge",
        "author": "Sean Hollister, Jay Peters",
        "date_pub": "2025-03-14T22:07:13Z",
        "contenu": "﻿‘I feel worried about this art form.’\n﻿‘I feel worried about this art form.’\nby  Sean Hollister and  Jay Peters\nAshly Burch, the award-winning voice and performance actor behind Horizon Zero Dawn’s Aloy — one of the most prominent characters on PlayStation today — has some news and some very strong thoughts about the leaked Sony experiment that saw her character voiced and performed by AI technology instead of her or any other human being.\nThe video, originally shared with The Verge by a tipster and later pulled off YouTube by a copyright enforcement company that counts Sony PlayStation as a client, was of an internal prototype — not necessarily something that’s in production for actual games. Burch says Horizon developer Guerilla proactively confirmed to her that it is not actively in development. Nor did it use her voice or facial data, Guerilla claimed. \nBut Burch says having seen the demo, she is worried, and not just about her own career. “I feel worried about this art form,” she says. You can watch her video immediately below, or scroll down for a transcript. \nThe transcript:\nHi. Let’s talk about AI Aloy. I saw the tech demo earlier this week. Guerilla reached out to me to let me know that the demo didn’t reflect anything that was actively in development. They didn’t use any of my performance for the demo, so none of my facial or voice data. And Guerilla owns Aloy as a character.\nSo all that said, I feel worried. And not worried about Guerilla specifically or Horizon or my performance or my career specifically, even. I feel worried about this art form. Game performance as an art form.\nWe are currently on strike. SAG-AFTRA is on strike against video games because of AI. Because this technology exists, because we know that game companies want to use it, we’re asking for protections.\nSo currently what we’re fighting for is that you have to get our consent before you make an AI version of us in any form. You have to compensate us fairly and you have to tell us how you’re using this AI double.\nAnd I feel worried not because the technology exists. Not even because game companies want to use it. Because of course they do. They always want to use technological advancements.\nI just imagine a video like this coming out that does have someone’s performance attached to it. That does have someone’s voice or face or movement. And the possibility that if we lose this fight, that person would have no recourse.\nThey wouldn’t have any protections. Any way to fight back. And that possibility… it makes me so sad. It hurts my heart. It scares me.\nI love this industry and this art form so much and I want there to be a new generation of actors. I want there to be so many more incredible game performances.\nI want to be able to continue, to do this job, and if we don’t win, then that future is really compromised.\nIn a slightly longer TikTok version of the video, which we’ve embedded above to replace the original Instagram copy, Burch adds that “I’m genuinely not trying to put any game company specifically on blast, certainly not Guerilla. The technology isn’t the problem. Game companies wanting to use the technology is not the problem. The problem is we’re currently on strike, and the bargaining group will not agree to give us common sense protections.”\nIt’s unusual for performers who have such a close relationship with game companies to speak out like this, but we’re also in an unusual moment: as she points out, video game actors are on strike right now, specifically because of AI, and the very idea that a company like Sony is explicitly building and demonstrating ways to potentially replace actors like Burch is exactly what the striking workers fear.\nIn addition to starring in Horizon Zero Dawn, Burch has had minor roles in other Sony games including The Last of Us Part II and Spider-Man, but is otherwise best known for playing Chloe Price in the Life Is Strange games, Tiny Tina in Borderlands, and from the live-action D&D roleplaying series Critical Role and Apple TV Plus’s Mythic Quest, where she also serves as a writer.\nUpdate, March 14th: Swapped Instagram video for Burch’s longer uncut TikTok video and added its additional context.\nA weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.\n© 2025 Vox Media, LLC. All Rights Reserved",
        "keywords": [
            "feel worried",
            "worried",
            "Jay Peters",
            "Sean Hollister",
            "Hollister and Jay",
            "Burch",
            "Guerilla",
            "Game",
            "feel",
            "video"
        ]
    },
   
    {
        "id": "2d544c54-6a41-4bc3-a3d3-536723f8f6e0",
        "type": "article",
        "domaine": "technology",
        "titre": "Undergraduate Upends a 40-Year-Old Data Science Conjecture",
        "url": "https://www.wired.com/story/undergraduate-upends-a-40-year-old-data-science-conjecture/",
        "description": "A young computer scientist and two colleagues show that searches within data structures called hash tables can be much faster than previously deemed possible.",
        "source": "Wired",
        "author": "Steve Nadis",
        "date_pub": "2025-03-16T11:00:00Z",
        "contenu": "The original version of this story appeared in Quanta Magazine.\nSometime in the fall of 2021, Andrew Krapivin, an undergraduate at Rutgers University, encountered a paper that would change his life. At the time, Krapivin didn’t give it much thought. But two years later, when he finally set aside time to go through the paper (“just for fun,” as he put it), his efforts would lead to a rethinking of a widely used tool in computer science.\nThe paper’s title, “Tiny Pointers,” referred to arrowlike entities that can direct you to a piece of information, or element, in a computer’s memory. Krapivin soon came up with a potential way to further miniaturize the pointers so they consumed less memory. However, to achieve that, he needed a better way of organizing the data that the pointers would point to.\nHe turned to a common approach for storing data known as a hash table. But in the midst of his tinkering, Krapivin realized that he had invented a new kind of hash table, one that worked faster than expected—taking less time and fewer steps to find specific elements.\nMartín Farach-Colton, a coauthor of the “Tiny Pointers” paper and Krapivin’s former professor at Rutgers, was initially skeptical of Krapivin’s new design. Hash tables are among the most thoroughly studied data structures in all of computer science; the advance sounded too good to be true. But just to be sure, he asked a frequent collaborator (and a “Tiny Pointers” coauthor), William Kuszmaul of Carnegie Mellon University, to check out his student’s invention. Kuszmaul had a different reaction. “You didn’t just come up with a cool hash table,” he remembers telling Krapivin. “You’ve actually completely wiped out a 40-year-old conjecture!”\nWithout setting out to do so, Andrew Krapivin upended the common thinking around hash tables—one of the best-studied tools in computer science.\nTogether, Krapivin (now a graduate student at the University of Cambridge), Farach-Colton (now at New York University), and Kuszmaul demonstrated in a January 2025 paper that this new hash table can indeed find elements faster than was considered possible. ln so doing, they had disproved a conjecture long held to be true.\n“It’s an important paper,” said Alex Conway of Cornell Tech in New York City. “Hash tables are among the oldest data structures we have. And they’re still one of the most efficient ways to store data.” Yet open questions remain about how they work, he said. “This paper answers a couple of them in surprising ways.”\nHash tables have become ubiquitous in computing, partly because of their simplicity and ease of use. They’re designed to allow users to do exactly three things: “query” (search for) an element, delete an element, or insert one into an empty slot. The first hash tables date back to the early 1950s, and computer scientists have studied and used them ever since. Among other things, researchers wanted to figure out the speed limits for some of these operations. How fast, for example, could a new search or insertion possibly be?\nMartín Farach-Colton helped Krapivin prove that his new hash table contradicted a long-standing conjecture.\nThe answer generally depends on the amount of time it takes to find an empty spot in a hash table. This, in turn, typically depends on how full the hash table is. Fullness can be described in terms of an overall percentage—this table is 50 percent full, that one’s 90 percent—but researchers often deal with much fuller tables. So instead, they may use a whole number, denoted by x, to specify how close the hash table is to 100 percent full. If x is 100, then the table is 99 percent full. If x is 1,000, the table is 99.9 percent full. This measure of fullness offers a convenient way to evaluate how long it should take to perform actions like queries or insertions.\nResearchers have long known that for certain common hash tables, the expected time required to make the worst possible insertion—putting an item into, say, the last remaining open spot—is proportional to x. “If your hash table is 99 percent full,” Kuszmaul said, “it makes sense that you would have to look at around 100 different positions to find a free slot.”\nIn a 1985 paper, the computer scientist Andrew Yao, who would go on to win the A.M. Turing Award, asserted that among hash tables with a specific set of properties, the best way to find an individual element or an empty spot is to just go through potential spots randomly—an approach known as uniform probing. He also stated that, in the worst-case scenario, where you’re searching for the last remaining open spot, you can never do better than x. For 40 years, most computer scientists assumed that Yao’s conjecture was true.\nKrapivin was not held back by the conventional wisdom for the simple reason that he was unaware of it. “I did this without knowing about Yao’s conjecture,” he said. His explorations with tiny pointers led to a new kind of hash table—one that did not rely on uniform probing. And for this new hash table, the time required for worst-case queries and insertions is proportional to (log x)2—far faster than x. This result directly contradicted Yao’s conjecture. Farach-Colton and Kuszmaul helped Krapivin show that (log x)2 is the optimal, unbeatable bound for the popular class of hash tables Yao had written about.\n“This result is beautiful in that it addresses and solves such a classic problem,” said Guy Blelloch of Carnegie Mellon.\n“It’s not just that they disproved [Yao’s conjecture], they also found the best possible answer to his question,” said Sepehr Assadi of the University of Waterloo. “We could have gone another 40 years before we knew the right answer.”\nKrapivin on the King’s College Bridge at the University of Cambridge. His new hash table can find and store data faster than researchers ever thought possible.\nIn addition to refuting Yao’s conjecture, the new paper also contains what many consider an even more astonishing result. It pertains to a related, though slightly different, situation: In 1985, Yao looked not only at the worst-case times for queries, but also at the average time taken across all possible queries. He proved that hash tables with certain properties—including those that are labeled “greedy,” which means that new elements must be placed in the first available spot—could never achieve an average time better than log x.\nFarach-Colton, Krapivin, and Kuszmaul wanted to see if that same limit also applied to non-greedy hash tables. They showed that it did not by providing a counterexample, a non-greedy hash table with an average query time that’s much, much better than log x. In fact, it doesn’t depend on x at all. “You get a number,” Farach-Colton said, “something that is just a constant and doesn’t depend on how full the hash table is.” The fact that you can achieve a constant average query time, regardless of the hash table’s fullness, was wholly unexpected—even to the authors themselves.\nThe team’s results may not lead to any immediate applications, but that’s not all that matters, Conway said. “It’s important to understand these kinds of data structures better. You don’t know when a result like this will unlock something that lets you do better in practice.”\nOriginal story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.\nIn your inbox: Upgrade your life with WIRED-tested gear\nHow to avoid US-based digital services, and why you might want to\nThe Big Story: Inside Elon Musk’s ‘digital coup’\n‘Airport theory’ will make you miss your flight\nSpecial Edition: How to get computers—before computers get you\nMore From WIRED\nReviews and Guides\n© 2025 Condé Nast. All rights reserved. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices",
        "keywords": [
            "hash table",
           
            "hash",
            "new hash table",
            "the hash table",
         
            "table",
            "Krapivin",
         
            "tables"
        ]
    },
    {
        "id": "533e04cf-08f7-4a96-ac7f-347ba4232c29",
        "type": "article",
        "domaine": "technology",
        "titre": "Report: Feds Put Notorious Anti-Vaxxer in Charge of Autism Study",
        "url": "https://gizmodo.com/report-feds-put-notorious-anti-vaxxer-in-charge-of-autism-study-2000580989",
        "description": "David Geier and his father Mark Geier have a long track record of misrepresenting the science on vaccine safety.",
        "source": "Gizmodo.com",
        "author": "Ed Cara",
        "date_pub": "2025-03-26T20:55:25Z",
        "contenu": "The wolves have firmly taken charge of the henhouse. The federal government has reportedly chosen David Geier, a notorious vaccine skeptic, to lead its upcoming study examining an already debunked link between vaccination and autism.\nThe Washington Post first reported on the development Tuesday afternoon, citing anonymous current and former federal health officials. Geier has long advocated against vaccination, often releasing shoddy and swiftly discredited research. His hiring is yet another sign that the country’s public health agenda, now led by Robert F. Kennedy Jr., is being led astray.\nNews of the federal government’s planned study into vaccines and autism first broke in early March. At the time, the CDC was expected to lead the research and began to do so at the behest of the U.S. Department of Health and Human Services, according to The Washington Post.\nIn recent weeks, however, HHS reportedly changed tack and instructed the National Institutes of Health to conduct the study instead. HHS has hired Geier to lead the analysis, according to the Washington Post. A listing matching his name can be seen on the agency’s employee directory, where he is described as a senior data analyst. When briefly interviewed by The Washington Post, Geier declined to confirm whether he had a role in the study, directing questions to those in charge.\nGizmodo reached out to HHS but has not yet received any comment regarding these latest developments.\nYou Thought Anti-Vax Was Bad? These Are RFK Jr.’s Most Disturbing Beliefs About Health\nMuch like HHS chief RFK Jr., Geier has a long history of supporting the anti-vaccination movement, usually alongside his father, Mark Geier. The Geiers have regularly released research purporting to show the hidden dangers of vaccination, often focusing on a link between mercury-based ingredients in vaccines and autism—research that Kennedy himself has cited in his attacks against vaccines.\nIn addition to his research, Mark Geier has frequently served as an expert legal witness for plaintiffs seeking compensation for alleged “vaccine injuries” in federal court cases, but he has been barred from doing so in some cases. The Geiers have also created and profited from the sale of autism treatments supposedly meant to help people damaged by mercury.\nImportantly, the vast majority of studies examining the issue have failed to find any connection between autism and vaccines or their ingredients. The Geiers’ research has also been routinely criticized by scientists over the years for misinterpreting data to support their agenda, and their studies have occasionally been retracted.\nMark Geier does have scientific training as a physician and geneticist, but David Geier only has a bachelor’s degree in biology. In 2011, the Maryland State Board of Physicians suspended and later revoked Mark Geier’s license to practice medicine in the state over allegations that he mistreated several autistic patients in his care; that same year, the state charged David Geier with practicing medicine without a license, alleging that he misrepresented himself as a doctor to his father’s patients (Geier claimed that his only role in the practice was administrative).\nIn securing his nomination to lead HHS, RFK Jr. assured his critics that he would have no issue acknowledging that vaccines don’t cause autism once presented with the scientific evidence showing as much. But Kennedy has a long track record of ignoring this evidence, even dismissing examples of it during a Senate hearing prior to his confirmation. Geier’s hiring is another strong indicator that any such discovery process will be rigged in favor of the anti-vaccination movement.\n\nAutismHHSRFK Jrvaccines \n\r\n          Get the best tech, science, and culture news in your inbox daily.\r\n        \n\r\n          News from the future, delivered to your present.\r\n        \n\r\n      Please select your desired newsletters and submit your email to upgrade your inbox.\r\n    \n\n          A children's hospital has reported several cases of children being treated for both measles and vitamin A toxicity.\n        \n\n          The long-time Republican and wellness grifter will likely be a rubber stamp to the Trump administration's worst impulses.\n        \n\n          The CDC is set to cut a third of its contracts spending within the next two weeks—the latest bit of dismantling orchestrated by the Trump administration.\n        \n\n          Most vaccinated people should still be highly protected against measles, but there are important exceptions.\n        \n\n          Vaccinated people's risk of dementia was reduced by 20% over a seven year period, researchers have found.\n        \n\n          Children's Health Defense published an anti-vaccine webpage that appeared almost identical to the CDC site.\n        \nWe may earn a commission when you buy through links on our sites.\n©2025 GIZMODO USA LLC. All rights reserved.\nMode\n\n                Follow us\n              \nMode\n\n                Follow us\n              ",
        "keywords": [
            "Geier",
            "Washington Post",
            "Mark Geier",
            "David Geier",
            "chosen David Geier",
            "health",
            "wolves have firmly",
            "HHS",
            "Washington",
            "revoked Mark Geier"
        ]
    },
    
    {
        "id": "26452c37-b760-4291-8c30-dcbb77a20355",
        "type": "article",
        "domaine": "technology",
        "titre": "UK Govt Data People Not Technical, Says Ex-Downing St Data Science Head",
        "url": "https://news.slashdot.org/story/25/03/28/184236/uk-govt-data-people-not-technical-says-ex-downing-st-data-science-head",
        "description": "An anonymous reader shares a report: A former director of data science at the UK prime minister's office has told MPs that people working with data in government are not typically technical and would be unlikely to get a similar job in the private sector. \n\nI…",
        "source": "Slashdot.org",
        "author": "msmash",
        "date_pub": "2025-03-28T18:35:00Z",
        "contenu": "\n\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\tPlease create an account to participate in the Slashdot moderation system\n\t\t\t\t\t\n\t\t\t\t\n\nNickname:\n\n\n\nPassword:\n\n\n\nNickname:\n\n\n\nPassword:\n\n\n\nThe Fine Print: The following comments are owned by whoever posted them.  We are not responsible for them in any way.\n\nFrom everything I have read, Britain, more than most countries, puts in charge non-technical people who are totally unqualified for either leadership or their positions. I suspect this has something to do with their \"Lords/peasants\" society/history, but there are probably people who frequent this Website that could enlighten us for the reasons much better than me. I'll just relate what happened to Reginald Victor (known as R.V.) Jones. He became the head of British MI6 Scientific Intelligence/Investigation \nPolitics and the civil service are one of the few places you can have a career having done fairly useless degrees such as PPE. If you've got a technical or scientific qualification you can do a lot better salary and job satisfaction wise in the private sector.\nWinning upper class twit of the year is still an important qualification, worth more than any science, engineering or maths degree. Plus there is a fierce streak on anti intellectualism here for technical subjects. You get the piss taken for being a spod at school and people will almost brag about being bad at maths or computers.\nThe US seems to vacillate between putting generals and actors in charge, with an occasional lawyer sneaking in. It's not a new phenomenon.\nThere may be more comments in this discussion. Without JavaScript enabled, you might want to turn on Classic Discussion System in your preferences instead.\nAgain and Again, NSO Group's Customers Keep Getting Their Spyware Operations Caught\nInside YouTube's Weird World Of Fake Movie Trailers\nFunction reject.",
        "keywords": [
            "moderation system Nickname",
            "Fine Print",
            "Slashdot moderation system",
            "Slashdot moderation",
            "Password",
            "system Nickname",
            "Nickname",
            "create an account",
            "account to participate",
            "Print"
        ]
    },
    {
        "id": "b88d5e15-54f8-45c8-b418-d5a40e1c4343",
        "type": "article",
        "domaine": "technology",
        "titre": "AV1 is supposed to make streaming better, so why isn’t everyone using it?",
        "url": "https://www.theverge.com/tech/635020/av1-streaming-netflix-youtube-google-adoption",
        "description": "When you jump into a video on YouTube or Netflix, a lot happens very quickly behind the scenes. Video data is rapidly downloaded to your device, which then has to unpack and normalize that information into a smooth, hiccup-free stream. The process of encoding…",
        "source": "The Verge",
        "author": "Emma Roth",
        "date_pub": "2025-04-02T23:39:53Z",
        "contenu": "Tech’s biggest players are all in on AV1, but competing codecs and technical limitations might be holding it back.\nTech’s biggest players are all in on AV1, but competing codecs and technical limitations might be holding it back.\nby  Emma Roth\nWhen you jump into a video on YouTube or Netflix, a lot happens very quickly behind the scenes. Video data is rapidly downloaded to your device, which then has to unpack and normalize that information into a smooth, hiccup-free stream. The process of encoding and decoding video data has changed greatly over the years, with H.264 (AVC) and its successor H.265 (HEVC) remaining two of the most widely used codecs for streaming.\nBut in 2015, tech giants including Netflix, Microsoft, Google, Amazon, and Meta banded together to develop video compression’s latest evolution: AV1. The companies, which are part of the overarching Alliance for Open Media (AOMedia), say the video codec is around 30 percent more efficient compared to other standards like HEVC and the Google-developed VP9, allowing it to deliver higher-quality video at a lower bandwidth. AOMedia also claims that it’s royalty-free, meaning streaming device makers and video providers shouldn’t have to pay patent holders for using the technology.\nThat all should have been enough for AV1 to take over the video landscape. But even with all these improvements and the backing of some of the biggest names in tech, the codec hasn’t become ubiquitous. Many major names in streaming, including Max, Peacock, and Paramount Plus, still haven’t adopted AV1.\nSince AV1’s debut in 2018, we’ve seen big players hop on board and use the codec for streaming high-resolution content in 4K and 8K. Google began testing AV1 on YouTube in 2018, while Netflix added support for AV1 in 2021. Amazon Prime Video also adopted AV1 in 2021, and the codec is used in Instagram Reels as well as for screensharing in Microsoft Teams. Discord launched support in 2023, and Twitch is working on its implementation. Browsers like Google Chrome, Safari, Microsoft Edge, and Firefox have adopted AV1, too.\nA flurry of devices have adopted AV1 decoders, including TVs, phones, and streaming devices\nAs for everyone else, there are a few reasons why they may not have adopted AV1 yet, and a simple one is hardware. For AV1 to work properly, a device has to have the hardware to support it — or otherwise run potentially resource-intensive software that can handle decoding AV1 content instead. \nWithin the past five years or so, a flurry of devices have adopted AV1 decoders, including TVs, phones, and streaming devices like the latest Amazon Fire TV Stick 4K Max. Chip makers like Nvidia, AMD, and Intel have launched GPUs with the tech. Meanwhile, Apple built an AV1 decoder into its iPhone with the launch of the iPhone 15 Pro in 2023, and it later added AV1 support across the entire iPhone 16 lineup last year. But not every device maker has been keen to adopt the AV1 codec, as Roku accused Google of coercing the company into supporting the standard in 2021, claiming it would drive up costs to consumers.\n“In order to get its best features, you have to accept a much higher encoding complexity,” Larry Pearlstein, an associate professor of electrical and computer engineering at the College of New Jersey, tells The Verge. “But there is also higher decoding complexity, and that is on the consumer end.”\nThere are solutions for devices that don’t have a dedicated AV1 hardware decoder, but they’re just not as efficient. Google, for instance, lets Android app developers enable dav1d, an AV1 decoder developed by VideoLAN. YouTube is just one of the apps that use dav1d, which allows it to beam AV1 videos to users on older or mid-range phones. However, some users on YouTube have reported issues with phone battery life following the implementation.\nRight now, AOMedia says around 95 percent of Netflix’s content is encoded with AV1, as opposed to 50 percent of videos on YouTube. “It’s always going to be the chicken and egg, right?” Hari Kalva, chair and professor of Florida Atlantic University’s department of electrical engineering and computer science, tells The Verge. “Who should build this technology before the [AV1] content exists, versus do they have enough players to play this content?”\nOther standards have emerged in the video compression space, too. VVC, also known as H.266, was developed by the Moving Picture Experts Group (MPEG) and the Video Coding Experts Group (VCEG) — the same groups behind HEVC and AVC. It was finalized in 2020 and is supposed to compress video using 50 percent less data compared to HEVC, a bit more than AV1’s promise of 30 percent efficiency savings. But, unlike AV1, VVC isn’t royalty-free.\nEven with more efficient video compression, AV1 comes with some tradeoffs that could hamper adoption. For one, compressing videos using AV1 takes more time and energy. “In order to get that higher compression, you have to spend more time getting there,” Pierre-Anthony Lemieux, the executive director of AOMedia, said during an interview with The Verge. “As codecs get more efficient, they require more power.”\nThough Lemieux says that AV1 implementers have agreed not to charge for the use of the codec, the group’s royalty-free claim might not be as clear-cut as it’s presented. For years, the companies that implement video compression codecs have had to pay a fee to use the standards, typically through patent pools. Patent pools allow companies to license a group of patents for a certain technology all at once, rather than having to negotiate with individual patent holders. Companies like the Via Licensing Alliance, Access Advance, and Sisvel manage access to patent pools for technologies like HEVC and VVC.\n“Video compression, in particular, is an area that has had many, many smart people working on it for a long time,” Robert Moore, the Principal of Moore IP Solutions, tells The Verge. “The innovations that those people have created are what I would call an IP thicket — basically a very, very challenging environment for anyone to develop technology that is a standard that’s commercially viable.”\n“Our members are working on the next big thing”\nSome pools have since emerged and are claiming royalties on patents used by AV1, as outlined by Streaming Media, with the most recent forming in January 2025. AOMedia responded to the news of the first licensing program in 2019, saying it was “founded to leave behind to leave behind the very environment that the announcement endorses and had settled ‘patent licensing terms up front.’”\nThe European Union also opened an investigation into AOMedia’s licensing policy in 2022 over concerns that its “mandatory royalty-free cross licensing” agreement could stifle innovation, as it could affect “innovators that were not a part of AOM at the time of the creation of the AV1 technical, but whose patents are deemed essential to (its) technical specifications,” according to Reuters. It closed its investigation in 2023 for “priority reasons.”\nThat uncertainty still isn’t stopping AOMedia and its adopters from plowing ahead with AV1 as the future of online streaming — and working on its potential successor. “AV1 is going to be here for forever, probably,” Lemieux says. “But, of course, our members are working on the next big thing, and I expect something later this year.”\nA weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.\n© 2025 Vox Media, LLC. All Rights Reserved",
        "keywords": [
            "video",
            "video compression",
            "streaming",
            "technical limitations",
            "Emma Roth",
            "codec",
            "competing codecs",
            "HEVC",
            "Verge",
            "technical"
        ]
    },
    {
        "id": "cf6974bf-4399-455c-995f-1bb7ad7e9de3",
        "type": "article",
        "domaine": "technology",
        "titre": "I was a data scientist at NASA. Here are 5 things to know before you enter the field as it evolves with AI.",
        "url": "https://www.businessinsider.com/data-science-high-demand-ai-what-to-know-2025-3",
        "description": "Data science is one of the few fields resilient to the current federal budget pauses and reductions, says data scientist Chris Mattmann.",
        "source": "Business Insider",
        "author": "Robin Madell",
        "date_pub": "2025-03-21T09:10:02Z",
        "contenu": "This as-told-to essay is based on a conversation with Chris Mattmann, a 44-year-old data scientist from La Canada Flintridge, California, who previously served as NASA Jet Propulsion Laboratory's chief technology and innovation officer and division manager of artificial intelligence, analytics, and innovation organization. Mattmann spent nearly 24 years at NASA before joining UCLA in June 2024 as chief data and artificial intelligence officer. The following has been edited for length and clarity.\nI got started in data science long before it was even known as \"data science.\" When I studied at the University of Southern California from 1998 to 2007, I worked on data architecture, data engineering, databases, and data systems. My biggest interest was how they were all interconnected.\nI started working at NASA as an academic part-time employee in January 2001. Soon after, I was hired full time as a data engineer and software engineer.\nI moved up at NASA's Jet Propulsion Laboratory (JPL) by working on missions, and had my big break while working on the Orbiting Carbon Observatory Mission, a next-generation earth science instrument. I became JPL's chief technology and innovation officer in 2020.\nWhen I entered the industry, I had a lot of training in software development and engineering.\nI saw that people coming out with data degrees were more effective as data scientists than the software developers who took years just to learn what latitude and longitude were; teaching an earth scientist Python was more effective than teaching a software engineering Ph.D. earth science.\nLooking back, I wish my first five years were spent learning earth science, planetary science, and more math rather than software development or engineering, which I could've picked up in greater detail later.\nI recommend that folks get a discipline science degree rather than a computer or software degree. AI is coming for your software engineering job, but it isn't the best at discipline sciences. Getting a degree in those areas will allow you to have a lengthier career.\nSomeone can enter this field in two main ways.\nThe first is by doing something interesting with open-source tools and data and then putting it on your GitHub for others to review and see, which proves you can do a real-world problem. Kaggle has many challenges like this where you can compete against others.\nThe second is by working or studying under a mentor or doing an internship, where you make a publicly reviewable contribution to data.\nFor me, the sweet spot is to carefully navigate both data research and operations; don't just hide in the research domain; instead, actually learn the software engineering necessary to deliver the application of research, data, and AI to customers.\nI find operations to be a much more rewarding and less cutthroat field than research and the science publishing community and pipeline. My work in operations has included everything from NASA missions focused on big data for earth science, to software like Apache Hadoop and Apache Tika, which are used around the world by tens of millions of people and companies.\nThe biggest challenges were preparing myself not to be in the lead, spending time working in the background on data analysis, and having others get most of the credit for the work I enabled.\nI often say I'm a \"little s\" scientist, not a \"big S\" scientist like the others — mostly because I was made to feel that way. I was constantly put as the second or last author on papers that I was the largest contributor to, so the earth or planetary scientist could get their first author paper at a major conference.\nYou're an \"assistant\" in many cases to the actual discipline scientist that you're preparing, analyzing, and working on the data for. You're also now the \"help\" for fueling AI, which is heavily data-dependent, so you have to humble yourself greatly.\nWhen you have a manager you really trust, you can try to move into data, AI research, and analytics, so you can share the credit without being worried about getting pushed to the side.\nCommunity in data science is so important. You need folks around you to lift you up and be interested in what you're doing.\nData science is definitely a team sport. While I did work with some scientists who belittled my work, I overall had a supportive system of people around me, including my family and my peers. If you don't have this and feel isolated, then get involved in data science competitions, go to meet-ups, and build your friend network and community.\nBefore deciding to pursue a career in data science, consider whether you enjoy analytical tasks or operations work and whether you prefer being a team player or a leader. A mix of both analytical and operations skills tends to lend itself to good leadership in data science.\nBe aware you can easily burn out in data science and get bored. This is especially true if you spend your time only in Juptyer Notebooks and data analysis. Having a support network helps to avoid boredom and burnout.\nMaking sure you have the opportunity to move between both sides of the data science pipeline — operations and research — also helps to avoid this burnout.\nData analysis will be replaced in the next five to 10 years, done better by AI than humans. But training new AI and refining data will still have a big place and you can focus on that.\nAI is data-hungry for fuel, and understanding math, statistics, and how to evaluate data science and AI will be much more important than building it.\nUnderstanding the legal and ethical implications of building AI and data models will also become very important. People are so overwhelmed with data and misinformation that you'll have to prepare to tell better data stories and to be a background player in AI.\nI'm working toward overcoming these challenges by thinking about how the job market will change. I see where the winds are headed and I'm getting ahead of them.\nData science across disciplines is certainly something that's been a calling for me.\nYou can find it nowadays across industry, government, commercial, and academia sectors. And there's still high demand for the skill and profession even in the age of AI because data is the fuel for AI.\nDespite the recent DOGE cuts, I wouldn't mind being a data scientist at NASA now. Data science is one of the few fields resilient to the current federal budget pauses and reductions. Being a data scientist positions you well, given this new government direction.\nDo you have a tech story to share? Contact this editor, Jane Zhang, at janezhang@businessinsider.com.\nJump to",
        "keywords": [
            "Canada Flintridge",
            "data",
            "data science",
            "NASA Jet Propulsion",
            "Chris Mattmann",
            "Jet Propulsion Laboratory",
            "science",
            "conversation with Chris",
            "NASA Jet",
            "NASA"
        ]
    }
  
   
]