# -*- coding: utf-8 -*-
"""articles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUPENPQPq-11X5jhyuuJBk7ECRbGT90W
"""

!pip install newsapi-python
!pip install beautifulsoup4
!pip install requests

import json
import uuid
import requests
from bs4 import BeautifulSoup
from newsapi import NewsApiClient

# Fonction de scraping pour obtenir le contenu texte de l'article
def get_article_content_scraping(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Essayer d'extraire le texte √† partir des balises <p>
        paragraphs = soup.find_all('p')
        texte = "\n".join([p.get_text() for p in paragraphs])

        return texte if texte else "Contenu non disponible"
    except Exception as e:
        return f"Erreur lors du scraping : {str(e)}"

# Fonction pour collecter les articles avec NewsAPI
def collecter_articles_newsapi(domaine, mots_cles, articles_par_mot=5):
    articles_total = []
    urls_vues = set()

    api = NewsApiClient(api_key='7f68a1a1c4d84cd59169037b4b8418ab')

    for mot in mots_cles:
        print(f"üîç Recherche pour : {mot}")
        resultats = api.get_everything(q=mot, language='en', page_size=articles_par_mot)

        for article in resultats['articles']:
            url = article.get('url')
            if url and url not in urls_vues:
                article_data = {
                    "id": str(uuid.uuid4()),
                    "type": "article",
                    "domaine": domaine,
                    "titre": article.get('title'),
                    "url": url,
                    "description": article.get('description', ""),
                    "source": article.get('source', {}).get('name'),
                    "author": article.get('author', "Inconnu"),
                    "date_pub": article.get('publishedAt', "Non sp√©cifi√©e"),
                    "contenu": get_article_content_scraping(url)
                }
                articles_total.append(article_data)
                urls_vues.add(url)

    return articles_total

# Mots-cl√©s par domaine
mots_cles_math = ["algebra", "calculus"]
mots_cles_science = ["physics", "chemistry"]
mots_cles_eng = ["mechanical engineering", "electrical engineering"]
mots_cles_tech = ["technology", "data science"]

# Collecter 20 √† 30 articles par domaine (5 mots x 5-6 articles)
articles_math = collecter_articles_newsapi("mathematics", mots_cles_math, articles_par_mot=10)
articles_science = collecter_articles_newsapi("science", mots_cles_science, articles_par_mot=10)
articles_eng = collecter_articles_newsapi("engineering", mots_cles_eng, articles_par_mot=10)
articles_tech = collecter_articles_newsapi("technology", mots_cles_tech, articles_par_mot=10)

# Sauvegarde dans des fichiers JSON
with open("articles_math.json", "w", encoding="utf-8") as f:
    json.dump(articles_math, f, indent=2, ensure_ascii=False)

with open("articles_science.json", "w", encoding="utf-8") as f:
    json.dump(articles_science, f, indent=2, ensure_ascii=False)

with open("articles_eng.json", "w", encoding="utf-8") as f:
    json.dump(articles_eng, f, indent=2, ensure_ascii=False)

with open("articles_tech.json", "w", encoding="utf-8") as f:
    json.dump(articles_tech, f, indent=2, ensure_ascii=False)

print("‚úÖ Articles collect√©s et enregistr√©s avec succ√®s !")

from google.colab import files
import json

# Sauvegarder les articles dans des fichiers JSON
with open("articles_math.json", "w", encoding="utf-8") as f:
    json.dump(articles_math, f, indent=2, ensure_ascii=False)

with open("articles_science.json", "w", encoding="utf-8") as f:
    json.dump(articles_science, f, indent=2, ensure_ascii=False)

with open("articles_eng.json", "w", encoding="utf-8") as f:
    json.dump(articles_eng, f, indent=2, ensure_ascii=False)

with open("articles_tech.json", "w", encoding="utf-8") as f:
    json.dump(articles_tech, f, indent=2, ensure_ascii=False)

# T√©l√©charger les fichiers g√©n√©r√©s
files.download("articles_math.json")
files.download("articles_science.json")
files.download("articles_eng.json")
files.download("articles_tech.json")